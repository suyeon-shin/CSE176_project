import lightgbm as lgb
import pandas as pd
from sklearn.decomposition import PCA
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import mean_squared_error
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split

data_file = pd.read_csv(r'C:\rbi_reg_cs_176\venv\train.csv')

y = data_file['critical_temp']

X = data_file.drop('critical_temp', axis=1)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Apply PCA to reduce the dimensionality of the data
pca = PCA(n_components=40)
X_train_pca = pca.fit_transform(X_train)
X_test_pca = pca.transform(X_test)

# Convert the data to LightGBM format
train_data = lgb.Dataset(X_train_pca, label=y_train)
valid_data = lgb.Dataset(X_test_pca, label=y_test, reference=train_data)

# Set the hyperparameters for the LightGBM model
params = {
    'objective': 'regression',
    'metric': 'rmse',
    'num_leaves': 30,
    'learning_rate': 0.05,
}

# Define the hyperparameters to search
hyperparameters = {
    'num_leaves': [10, 20, 30, 40, 50],
    'learning_rate': [0.01, 0.05, 0.1, 0.2],
    'n_estimators': [100, 500, 1000, 2000]
}

# Create a LightGBM model
model = lgb.LGBMRegressor()

# Use GridSearchCV to search for the best hyperparameters
grid_search = GridSearchCV(
    model,
    hyperparameters,
    cv=5,
    scoring='neg_mean_squared_error',
    n_jobs=-1,
)

grid_search.fit(X_train_pca, y_train, eval_set=[(X_test_pca, y_test)], eval_metric='rmse', early_stopping_rounds=10)

# Print the best hyperparameters found during training
best_params = grid_search.best_params_
print(f'Best hyperparameters: {best_params}')

# Train a new LightGBM model with the best hyperparameters
model = lgb.LGBMRegressor(**best_params)
model.fit(X_train_pca, y_train)

# Evaluate the model on the test set
y_pred = model.predict(X_test_pca)
accuracy = mean_squared_error(y_test, y_pred, squared=False)
print(f'Test accuracy: {accuracy}')
# was testing pca theories on this file
